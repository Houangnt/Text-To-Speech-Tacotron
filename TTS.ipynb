{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FSWZVNzT_q5",
        "outputId": "cf2e9ff4-a5ee-4f88-ce17-84747bacacac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdaQAfIyUh-a",
        "outputId": "0d0a494b-fc88-4dd0-f6cc-dce2df674ed1"
      },
      "source": [
        "!git clone https://github.com/fatchord/WaveRNN.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'WaveRNN'...\n",
            "remote: Enumerating objects: 928, done.\u001b[K\n",
            "remote: Total 928 (delta 0), reused 0 (delta 0), pack-reused 928\u001b[K\n",
            "Receiving objects: 100% (928/928), 242.13 MiB | 2.64 MiB/s, done.\n",
            "Resolving deltas: 100% (525/525), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFQoQqiOUmxS",
        "outputId": "c711d973-db32-4fd6-e017-c022b11843cd"
      },
      "source": [
        "%cd WaveRNN/\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/WaveRNN\n",
            "Collecting numpy==1.16.2\n",
            "  Downloading numpy-1.16.2-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 3.7 MB/s \n",
            "\u001b[?25hCollecting librosa==0.6.3\n",
            "  Downloading librosa-0.6.3.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.2.2)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 68.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.2.5)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.3->-r requirements.txt (line 2)) (2.1.9)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.3->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.3->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.3->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.3->-r requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.3->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.3->-r requirements.txt (line 2)) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.3->-r requirements.txt (line 2)) (0.51.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.38.0->librosa==0.6.3->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.38.0->librosa==0.6.3->-r requirements.txt (line 2)) (0.34.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.6.3->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.2)\n",
            "Building wheels for collected packages: librosa\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.6.3-py3-none-any.whl size=1573336 sha256=d3c1063a2415b3be6f9af34bc49ed0077a948cc1cdc542fdfe5cfda3fb21e8be\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c1/94/619fb8b04ee1f567115662d26650677ecf79bc7d8e462d21f8\n",
            "Successfully built librosa\n",
            "Installing collected packages: numpy, unidecode, librosa\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.1\n",
            "    Uninstalling librosa-0.8.1:\n",
            "      Successfully uninstalled librosa-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 0.18.2 requires numpy>=1.17, but you have numpy 1.16.2 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.16.2 which is incompatible.\n",
            "pywavelets 1.2.0 requires numpy>=1.17.3, but you have numpy 1.16.2 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.2 which is incompatible.\n",
            "pyarrow 3.0.0 requires numpy>=1.16.6, but you have numpy 1.16.2 which is incompatible.\n",
            "kapre 0.3.6 requires librosa>=0.7.2, but you have librosa 0.6.3 which is incompatible.\n",
            "kapre 0.3.6 requires numpy>=1.18.5, but you have numpy 1.16.2 which is incompatible.\n",
            "jaxlib 0.1.74+cuda11.cudnn805 requires numpy>=1.18, but you have numpy 1.16.2 which is incompatible.\n",
            "jax 0.2.25 requires numpy>=1.18, but you have numpy 1.16.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "cupy-cuda111 9.4.0 requires numpy<1.24,>=1.17, but you have numpy 1.16.2 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.2 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed librosa-0.6.3 numpy-1.16.2 unidecode-1.3.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvU9wH0MVM62",
        "outputId": "225caa2a-c684-4490-f2d0-ee8e16bcb52f"
      },
      "source": [
        "!pip install numba==0.48"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numba==0.48\n",
            "  Downloading numba-0.48.0-1-cp37-cp37m-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.48) (57.4.0)\n",
            "Collecting llvmlite<0.32.0,>=0.31.0dev0\n",
            "  Downloading llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba==0.48) (1.16.2)\n",
            "Installing collected packages: llvmlite, numba\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.6 requires librosa>=0.7.2, but you have librosa 0.6.3 which is incompatible.\n",
            "kapre 0.3.6 requires numpy>=1.18.5, but you have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "Successfully installed llvmlite-0.31.0 numba-0.48.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd7r_uyRUs7h",
        "outputId": "15b54f9c-2b68-4a04-ab99-4dc018b577aa"
      },
      "source": [
        "!python preprocess.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "13100 wav files found in \"/content/drive/MyDrive/NLP/LJSpeech-1.1/\"\n",
            "\n",
            "+-------------+-----------+--------+------------+-----------+\n",
            "| Sample Rate | Bit Depth | Mu Law | Hop Length | CPU Usage |\n",
            "+-------------+-----------+--------+------------+-----------+\n",
            "|    22050    |     9     |  True  |    275     |    3/4    |\n",
            "+-------------+-----------+--------+------------+-----------+\n",
            " \n",
            "████████████████ 13100/13100 \n",
            "\n",
            "Completed. Ready to run \"python train_tacotron.py\" or \"python train_wavernn.py\". \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1I1Q3WopLeF",
        "outputId": "887392ec-8abf-477e-dd79-4e60a0d9053b"
      },
      "source": [
        "!python train_tacotron.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Initialising Tacotron Model...\n",
            "\n",
            "Trainable Parameters: 11.088M\n",
            "Creating latest checkpoint...\n",
            "Saving latest weights: /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_weights.pyt\n",
            "Saving latest optimizer state: /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_optim.pyt\n",
            "+----------------+------------+---------------+------------------+\n",
            "| Steps with r=7 | Batch Size | Learning Rate | Outputs/Step (r) |\n",
            "+----------------+------------+---------------+------------------+\n",
            "|   10k Steps    |     32     |     0.001     |        7         |\n",
            "+----------------+------------+---------------+------------------+\n",
            " \n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "| Epoch: 1/25 (410/410) | Loss: 1.399 | 2.2 steps/s | Step: 0k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 2/25 (410/410) | Loss: 1.248 | 2.3 steps/s | Step: 0k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 3/25 (410/410) | Loss: 1.196 | 2.3 steps/s | Step: 1k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 4/25 (410/410) | Loss: 1.163 | 2.3 steps/s | Step: 1k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 5/25 (410/410) | Loss: 1.142 | 2.3 steps/s | Step: 2k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 6/25 (410/410) | Loss: 1.124 | 2.3 steps/s | Step: 2k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 7/25 (410/410) | Loss: 1.109 | 2.3 steps/s | Step: 2k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 8/25 (410/410) | Loss: 1.098 | 2.3 steps/s | Step: 3k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 9/25 (410/410) | Loss: 1.087 | 2.3 steps/s | Step: 3k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 10/25 (410/410) | Loss: 1.078 | 2.2 steps/s | Step: 4k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 11/25 (410/410) | Loss: 1.070 | 2.2 steps/s | Step: 4k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 12/25 (410/410) | Loss: 1.063 | 2.2 steps/s | Step: 4k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 13/25 (410/410) | Loss: 1.056 | 2.2 steps/s | Step: 5k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 14/25 (410/410) | Loss: 1.050 | 2.2 steps/s | Step: 5k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 15/25 (410/410) | Loss: 1.044 | 2.2 steps/s | Step: 6k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 16/25 (410/410) | Loss: 1.039 | 2.2 steps/s | Step: 6k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 17/25 (410/410) | Loss: 1.033 | 2.2 steps/s | Step: 6k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 18/25 (410/410) | Loss: 1.027 | 2.2 steps/s | Step: 7k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 19/25 (410/410) | Loss: 1.023 | 2.2 steps/s | Step: 7k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 20/25 (410/410) | Loss: 1.016 | 2.2 steps/s | Step: 8k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 21/25 (410/410) | Loss: 0.9945 | 2.2 steps/s | Step: 8k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 22/25 (410/410) | Loss: 0.9600 | 2.2 steps/s | Step: 9k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 23/25 (410/410) | Loss: 0.9391 | 2.2 steps/s | Step: 9k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 24/25 (410/410) | Loss: 0.9223 | 2.2 steps/s | Step: 9k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 25/25 (410/410) | Loss: 0.9091 | 2.2 steps/s | Step: 10k |  \n",
            "+----------------+------------+---------------+------------------+\n",
            "| Steps with r=5 | Batch Size | Learning Rate | Outputs/Step (r) |\n",
            "+----------------+------------+---------------+------------------+\n",
            "|   19k Steps    |     32     |    0.0001     |        5         |\n",
            "+----------------+------------+---------------+------------------+\n",
            " \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 1/49 (410/410) | Loss: 0.8298 | 1.7 steps/s | Step: 10k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 2/49 (410/410) | Loss: 0.8166 | 1.8 steps/s | Step: 11k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 3/49 (410/410) | Loss: 0.8115 | 1.7 steps/s | Step: 11k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 4/49 (410/410) | Loss: 0.8076 | 1.8 steps/s | Step: 11k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 5/49 (410/410) | Loss: 0.8047 | 1.8 steps/s | Step: 12k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 6/49 (410/410) | Loss: 0.8018 | 1.8 steps/s | Step: 12k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 7/49 (410/410) | Loss: 0.7992 | 1.8 steps/s | Step: 13k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 8/49 (410/410) | Loss: 0.7969 | 1.8 steps/s | Step: 13k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 9/49 (410/410) | Loss: 0.7945 | 1.8 steps/s | Step: 13k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 10/49 (410/410) | Loss: 0.7926 | 1.8 steps/s | Step: 14k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 11/49 (410/410) | Loss: 0.7903 | 1.8 steps/s | Step: 14k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 12/49 (410/410) | Loss: 0.7886 | 1.8 steps/s | Step: 15k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 13/49 (410/410) | Loss: 0.7868 | 1.8 steps/s | Step: 15k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 14/49 (410/410) | Loss: 0.7851 | 1.8 steps/s | Step: 15k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 15/49 (410/410) | Loss: 0.7834 | 1.7 steps/s | Step: 16k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 16/49 (410/410) | Loss: 0.7818 | 1.7 steps/s | Step: 16k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 17/49 (410/410) | Loss: 0.7803 | 1.7 steps/s | Step: 17k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 18/49 (410/410) | Loss: 0.7789 | 1.7 steps/s | Step: 17k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 19/49 (410/410) | Loss: 0.7773 | 1.7 steps/s | Step: 18k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 20/49 (410/410) | Loss: 0.7759 | 1.7 steps/s | Step: 18k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 21/49 (410/410) | Loss: 0.7745 | 1.7 steps/s | Step: 18k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 22/49 (410/410) | Loss: 0.7734 | 1.7 steps/s | Step: 19k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 23/49 (410/410) | Loss: 0.7719 | 1.7 steps/s | Step: 19k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 24/49 (410/410) | Loss: 0.7709 | 1.7 steps/s | Step: 20k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 25/49 (410/410) | Loss: 0.7695 | 1.7 steps/s | Step: 20k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 26/49 (410/410) | Loss: 0.7682 | 1.7 steps/s | Step: 20k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 27/49 (410/410) | Loss: 0.7672 | 1.7 steps/s | Step: 21k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 28/49 (410/410) | Loss: 0.7661 | 1.7 steps/s | Step: 21k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 29/49 (410/410) | Loss: 0.7648 | 1.7 steps/s | Step: 22k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 30/49 (410/410) | Loss: 0.7639 | 1.7 steps/s | Step: 22k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 31/49 (410/410) | Loss: 0.7627 | 1.7 steps/s | Step: 22k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 32/49 (410/410) | Loss: 0.7620 | 1.7 steps/s | Step: 23k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 33/49 (410/410) | Loss: 0.7609 | 1.7 steps/s | Step: 23k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 34/49 (410/410) | Loss: 0.7598 | 1.7 steps/s | Step: 24k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 35/49 (410/410) | Loss: 0.7588 | 1.7 steps/s | Step: 24k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 36/49 (410/410) | Loss: 0.7580 | 1.7 steps/s | Step: 25k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 37/49 (410/410) | Loss: 0.7568 | 1.7 steps/s | Step: 25k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 38/49 (410/410) | Loss: 0.7559 | 1.7 steps/s | Step: 25k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 39/49 (410/410) | Loss: 0.7552 | 1.7 steps/s | Step: 26k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 40/49 (410/410) | Loss: 0.7544 | 1.7 steps/s | Step: 26k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 41/49 (410/410) | Loss: 0.7533 | 1.7 steps/s | Step: 27k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 42/49 (410/410) | Loss: 0.7526 | 1.7 steps/s | Step: 27k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 43/49 (410/410) | Loss: 0.7517 | 1.7 steps/s | Step: 27k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 44/49 (410/410) | Loss: 0.7508 | 1.7 steps/s | Step: 28k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 45/49 (410/410) | Loss: 0.7498 | 1.7 steps/s | Step: 28k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 46/49 (410/410) | Loss: 0.7492 | 1.7 steps/s | Step: 29k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 47/49 (410/410) | Loss: 0.7483 | 1.7 steps/s | Step: 29k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 48/49 (410/410) | Loss: 0.7478 | 1.7 steps/s | Step: 29k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 49/49 (410/410) | Loss: 0.7467 | 1.7 steps/s | Step: 30k |  \n",
            "+----------------+------------+---------------+------------------+\n",
            "| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |\n",
            "+----------------+------------+---------------+------------------+\n",
            "|   19k Steps    |     16     |    0.0001     |        2         |\n",
            "+----------------+------------+---------------+------------------+\n",
            " \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 1/25 (819/819) | Loss: 0.6634 | 0.83 steps/s | Step: 31k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 2/25 (819/819) | Loss: 0.6447 | 0.83 steps/s | Step: 31k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 3/25 (819/819) | Loss: 0.6366 | 0.83 steps/s | Step: 32k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 4/25 (819/819) | Loss: 0.6325 | 0.83 steps/s | Step: 33k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 5/25 (819/819) | Loss: 0.6292 | 0.83 steps/s | Step: 34k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 6/25 (819/819) | Loss: 0.6265 | 0.83 steps/s | Step: 35k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 7/25 (819/819) | Loss: 0.6241 | 0.83 steps/s | Step: 36k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 8/25 (819/819) | Loss: 0.6218 | 0.83 steps/s | Step: 36k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 9/25 (819/819) | Loss: 0.6198 | 0.84 steps/s | Step: 37k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 10/25 (819/819) | Loss: 0.6179 | 0.83 steps/s | Step: 38k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 11/25 (819/819) | Loss: 0.6163 | 0.83 steps/s | Step: 39k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 12/25 (819/819) | Loss: 0.6144 | 0.84 steps/s | Step: 40k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 13/25 (819/819) | Loss: 0.6132 | 0.84 steps/s | Step: 40k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 14/25 (819/819) | Loss: 0.6118 | 0.84 steps/s | Step: 41k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 15/25 (819/819) | Loss: 0.6103 | 0.84 steps/s | Step: 42k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 16/25 (819/819) | Loss: 0.6089 | 0.84 steps/s | Step: 43k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 17/25 (819/819) | Loss: 0.6076 | 0.84 steps/s | Step: 44k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 18/25 (819/819) | Loss: 0.6066 | 0.84 steps/s | Step: 45k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 19/25 (819/819) | Loss: 0.6053 | 0.84 steps/s | Step: 45k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 20/25 (819/819) | Loss: 0.6042 | 0.85 steps/s | Step: 46k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 21/25 (819/819) | Loss: 0.6031 | 0.84 steps/s | Step: 47k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 22/25 (819/819) | Loss: 0.6020 | 0.84 steps/s | Step: 48k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 23/25 (819/819) | Loss: 0.6010 | 0.84 steps/s | Step: 49k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 24/25 (819/819) | Loss: 0.5999 | 0.84 steps/s | Step: 49k |  \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "| Epoch: 25/25 (819/819) | Loss: 0.5991 | 0.84 steps/s | Step: 50k |  \n",
            "Training Complete.\n",
            "To continue training increase tts_total_steps in hparams.py or use --force_train\n",
            "\n",
            "Creating Ground Truth Aligned Dataset...\n",
            "\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "████████████████ 1638/1638 Batches \n",
            "\n",
            "You can now train WaveRNN on GTA features - use python train_wavernn.py --gta\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsQ1kjs_MZvO",
        "outputId": "87f58b19-1467-4c3a-86ef-2493f336638e"
      },
      "source": [
        "!python train_tacotron.py --force_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Initialising Tacotron Model...\n",
            "\n",
            "Trainable Parameters: 11.088M\n",
            "Restoring from latest checkpoint...\n",
            "Loading latest weights: /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_weights.pyt\n",
            "Loading latest optimizer state: /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_optim.pyt\n",
            "+----------------+------------+---------------+------------------+\n",
            "| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |\n",
            "+----------------+------------+---------------+------------------+\n",
            "|   50k Steps    |     8      |    0.0001     |        2         |\n",
            "+----------------+------------+---------------+------------------+\n",
            " \n",
            "| Epoch: 1/31 (1638/1638) | Loss: 0.6020 | 0.74 steps/s | Step: 52k |  \n",
            "| Epoch: 2/31 (1638/1638) | Loss: 0.6003 | 0.74 steps/s | Step: 54k |  \n",
            "| Epoch: 3/31 (1638/1638) | Loss: 0.5988 | 0.74 steps/s | Step: 55k |  \n",
            "| Epoch: 4/31 (1638/1638) | Loss: 0.5976 | 0.74 steps/s | Step: 57k |  \n",
            "| Epoch: 5/31 (1638/1638) | Loss: 0.5964 | 0.74 steps/s | Step: 59k |  \n",
            "| Epoch: 6/31 (1638/1638) | Loss: 0.5952 | 0.74 steps/s | Step: 60k |  \n",
            "| Epoch: 7/31 (1638/1638) | Loss: 0.5942 | 0.75 steps/s | Step: 62k |  \n",
            "| Epoch: 8/31 (1638/1638) | Loss: 0.5932 | 0.74 steps/s | Step: 63k |  \n",
            "| Epoch: 9/31 (1638/1638) | Loss: 0.5919 | 0.74 steps/s | Step: 65k |  \n",
            "| Epoch: 10/31 (1638/1638) | Loss: 0.5911 | 0.74 steps/s | Step: 67k |  \n",
            "| Epoch: 11/31 (1638/1638) | Loss: 0.5899 | 0.75 steps/s | Step: 68k |  \n",
            "| Epoch: 12/31 (1638/1638) | Loss: 0.5891 | 0.75 steps/s | Step: 70k |  \n",
            "| Epoch: 13/31 (1638/1638) | Loss: 0.5881 | 0.75 steps/s | Step: 72k |  \n",
            "| Epoch: 14/31 (1638/1638) | Loss: 0.5873 | 0.76 steps/s | Step: 73k |  \n",
            "| Epoch: 15/31 (1638/1638) | Loss: 0.5863 | 0.74 steps/s | Step: 75k |  \n",
            "| Epoch: 16/31 (1638/1638) | Loss: 0.5856 | 0.74 steps/s | Step: 77k |  \n",
            "| Epoch: 17/31 (1638/1638) | Loss: 0.5846 | 0.75 steps/s | Step: 78k |  \n",
            "| Epoch: 18/31 (1638/1638) | Loss: 0.5839 | 0.74 steps/s | Step: 80k |  \n",
            "| Epoch: 19/31 (1638/1638) | Loss: 0.5831 | 0.74 steps/s | Step: 81k |  \n",
            "| Epoch: 20/31 (1638/1638) | Loss: 0.5823 | 0.74 steps/s | Step: 83k |  \n",
            "| Epoch: 21/31 (1638/1638) | Loss: 0.5815 | 0.74 steps/s | Step: 85k |  \n",
            "| Epoch: 22/31 (1638/1638) | Loss: 0.5808 | 0.74 steps/s | Step: 86k |  \n",
            "| Epoch: 23/31 (1638/1638) | Loss: 0.5800 | 0.73 steps/s | Step: 88k |  \n",
            "| Epoch: 24/31 (14/1638) | Loss: 0.5788 | 0.69 steps/s | Step: 88k | "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtzyjzLEF8Ps",
        "outputId": "f076b1f9-afaf-405e-bdda-560e75ebbb02"
      },
      "source": [
        "!python train_tacotron.py --force_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Initialising Tacotron Model...\n",
            "\n",
            "Trainable Parameters: 11.088M\n",
            "Restoring from latest checkpoint...\n",
            "Loading latest weights: /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_weights.pyt\n",
            "Loading latest optimizer state: /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_optim.pyt\n",
            "+----------------+------------+---------------+------------------+\n",
            "| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |\n",
            "+----------------+------------+---------------+------------------+\n",
            "|   50k Steps    |     8      |    0.0001     |        2         |\n",
            "+----------------+------------+---------------+------------------+\n",
            " \n",
            "| Epoch: 1/31 (1638/1638) | Loss: 0.5805 | 1.2 steps/s | Step: 87k |  \n",
            "| Epoch: 2/31 (1638/1638) | Loss: 0.5796 | 1.2 steps/s | Step: 89k |  \n",
            "| Epoch: 3/31 (1638/1638) | Loss: 0.5790 | 1.2 steps/s | Step: 90k |  \n",
            "| Epoch: 4/31 (1638/1638) | Loss: 0.5784 | 1.2 steps/s | Step: 92k |  \n",
            "| Epoch: 5/31 (1638/1638) | Loss: 0.5777 | 1.2 steps/s | Step: 94k |  \n",
            "| Epoch: 6/31 (1638/1638) | Loss: 0.5769 | 1.2 steps/s | Step: 95k |  \n",
            "| Epoch: 7/31 (1638/1638) | Loss: 0.5761 | 1.2 steps/s | Step: 97k |  \n",
            "| Epoch: 8/31 (1638/1638) | Loss: 0.5757 | 1.2 steps/s | Step: 99k |  \n",
            "| Epoch: 9/31 (1638/1638) | Loss: 0.5748 | 1.2 steps/s | Step: 100k |  \n",
            "| Epoch: 10/31 (1638/1638) | Loss: 0.5744 | 1.2 steps/s | Step: 102k |  \n",
            "| Epoch: 11/31 (1638/1638) | Loss: 0.5737 | 1.2 steps/s | Step: 104k |  \n",
            "| Epoch: 12/31 (1638/1638) | Loss: 0.5732 | 1.2 steps/s | Step: 105k |  \n",
            "| Epoch: 13/31 (1638/1638) | Loss: 0.5727 | 1.2 steps/s | Step: 107k |  \n",
            "| Epoch: 14/31 (1638/1638) | Loss: 0.5719 | 1.2 steps/s | Step: 108k |  \n",
            "| Epoch: 15/31 (1638/1638) | Loss: 0.5713 | 1.2 steps/s | Step: 110k |  \n",
            "| Epoch: 16/31 (1638/1638) | Loss: 0.5709 | 1.2 steps/s | Step: 112k |  \n",
            "| Epoch: 17/31 (1638/1638) | Loss: 0.5703 | 1.2 steps/s | Step: 113k |  \n",
            "| Epoch: 18/31 (1638/1638) | Loss: 0.5697 | 1.2 steps/s | Step: 115k |  \n",
            "| Epoch: 19/31 (1638/1638) | Loss: 0.5692 | 1.2 steps/s | Step: 117k |  \n",
            "| Epoch: 20/31 (1638/1638) | Loss: 0.5687 | 1.2 steps/s | Step: 118k |  \n",
            "| Epoch: 21/31 (1638/1638) | Loss: 0.5681 | 1.2 steps/s | Step: 120k |  \n",
            "| Epoch: 22/31 (1638/1638) | Loss: 0.5676 | 1.2 steps/s | Step: 122k |  \n",
            "| Epoch: 23/31 (1638/1638) | Loss: 0.5671 | 1.2 steps/s | Step: 123k |  \n",
            "| Epoch: 24/31 (1638/1638) | Loss: 0.5668 | 1.2 steps/s | Step: 125k |  \n",
            "| Epoch: 25/31 (1638/1638) | Loss: 0.5662 | 1.2 steps/s | Step: 126k |  \n",
            "| Epoch: 26/31 (786/1638) | Loss: 0.5653 | 1.2 steps/s | Step: 127k | Traceback (most recent call last):\n",
            "  File \"train_tacotron.py\", line 201, in <module>\n",
            "  File \"train_tacotron.py\", line 98, in main\n",
            "    \n",
            "  File \"train_tacotron.py\", line 142, in tts_train_loop\n",
            "    if hp.tts_clip_grad_norm is not None:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 307, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 156, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQLpU3_TXkL1",
        "outputId": "0a01ec5c-439f-46cc-9161-393246739773"
      },
      "source": [
        "!python train_tacotron.py --force_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Initialising Tacotron Model...\n",
            "\n",
            "Trainable Parameters: 11.088M\n",
            "Restoring from latest checkpoint...\n",
            "Loading latest weights: /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_weights.pyt\n",
            "Loading latest optimizer state: /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_optim.pyt\n",
            "+----------------+------------+---------------+------------------+\n",
            "| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |\n",
            "+----------------+------------+---------------+------------------+\n",
            "|   50k Steps    |     8      |    0.0001     |        2         |\n",
            "+----------------+------------+---------------+------------------+\n",
            " \n",
            "| Epoch: 1/31 (1638/1638) | Loss: 0.5658 | 1.2 steps/s | Step: 127k |  \n",
            "| Epoch: 2/31 (1638/1638) | Loss: 0.5656 | 1.2 steps/s | Step: 129k |  \n",
            "| Epoch: 3/31 (1638/1638) | Loss: 0.5652 | 1.1 steps/s | Step: 130k |  \n",
            "| Epoch: 4/31 (1638/1638) | Loss: 0.5646 | 1.1 steps/s | Step: 132k |  \n",
            "| Epoch: 5/31 (1638/1638) | Loss: 0.5641 | 1.1 steps/s | Step: 134k |  \n",
            "| Epoch: 6/31 (1638/1638) | Loss: 0.5637 | 1.1 steps/s | Step: 135k |  \n",
            "| Epoch: 7/31 (1638/1638) | Loss: 0.5633 | 1.1 steps/s | Step: 137k |  \n",
            "| Epoch: 8/31 (1638/1638) | Loss: 0.5629 | 1.2 steps/s | Step: 139k |  \n",
            "| Epoch: 9/31 (1638/1638) | Loss: 0.5623 | 1.2 steps/s | Step: 140k |  \n",
            "| Epoch: 10/31 (1638/1638) | Loss: 0.5617 | 1.2 steps/s | Step: 142k |  \n",
            "| Epoch: 11/31 (1638/1638) | Loss: 0.5615 | 1.1 steps/s | Step: 144k |  \n",
            "| Epoch: 12/31 (1638/1638) | Loss: 0.5611 | 1.2 steps/s | Step: 145k |  \n",
            "| Epoch: 13/31 (1638/1638) | Loss: 0.5607 | 1.2 steps/s | Step: 147k |  \n",
            "| Epoch: 14/31 (1638/1638) | Loss: 0.5602 | 1.2 steps/s | Step: 148k |  \n",
            "| Epoch: 15/31 (1638/1638) | Loss: 0.5599 | 1.2 steps/s | Step: 150k |  \n",
            "| Epoch: 16/31 (1638/1638) | Loss: 0.5595 | 1.2 steps/s | Step: 152k |  \n",
            "| Epoch: 17/31 (1638/1638) | Loss: 0.5592 | 1.2 steps/s | Step: 153k |  \n",
            "| Epoch: 18/31 (1638/1638) | Loss: 0.5588 | 1.2 steps/s | Step: 155k |  \n",
            "| Epoch: 19/31 (31/1638) | Loss: 0.5580 | 0.99 steps/s | Step: 155k | Traceback (most recent call last):\n",
            "  File \"train_tacotron.py\", line 202, in <module>\n",
            "    main()\n",
            "  File \"train_tacotron.py\", line 99, in main\n",
            "    tts_train_loop(paths, model, optimizer, train_set, lr, training_steps, attn_example)\n",
            "  File \"train_tacotron.py\", line 143, in tts_train_loop\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 307, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 156, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aTMpsrDrzCf",
        "outputId": "c4ce0f71-19e8-428b-e15b-5ae5c8fa981f"
      },
      "source": [
        "!python train_tacotron.py --force_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Initialising Tacotron Model...\n",
            "\n",
            "Trainable Parameters: 11.088M\n",
            "Restoring from latest checkpoint...\n",
            "Loading latest weights: /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_weights.pyt\n",
            "Loading latest optimizer state: /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_optim.pyt\n",
            "+----------------+------------+---------------+------------------+\n",
            "| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |\n",
            "+----------------+------------+---------------+------------------+\n",
            "|   21k Steps    |     16     |    0.0001     |        2         |\n",
            "+----------------+------------+---------------+------------------+\n",
            " \n",
            "| Epoch: 1/27 (819/819) | Loss: 0.5543 | 0.80 steps/s | Step: 158k |  \n",
            "| Epoch: 2/27 (819/819) | Loss: 0.5540 | 0.81 steps/s | Step: 159k |  \n",
            "| Epoch: 3/27 (819/819) | Loss: 0.5536 | 0.82 steps/s | Step: 160k |  \n",
            "| Epoch: 4/27 (819/819) | Loss: 0.5531 | 0.83 steps/s | Step: 161k |  \n",
            "| Epoch: 5/27 (819/819) | Loss: 0.5529 | 0.82 steps/s | Step: 162k |  \n",
            "| Epoch: 6/27 (819/819) | Loss: 0.5527 | 0.82 steps/s | Step: 163k |  \n",
            "| Epoch: 7/27 (819/819) | Loss: 0.5525 | 0.82 steps/s | Step: 163k |  \n",
            "| Epoch: 8/27 (819/819) | Loss: 0.5523 | 0.81 steps/s | Step: 164k |  \n",
            "| Epoch: 9/27 (819/819) | Loss: 0.5520 | 0.81 steps/s | Step: 165k |  \n",
            "| Epoch: 10/27 (819/819) | Loss: 0.5518 | 0.80 steps/s | Step: 166k |  \n",
            "| Epoch: 11/27 (819/819) | Loss: 0.5515 | 0.81 steps/s | Step: 167k |  \n",
            "| Epoch: 12/27 (819/819) | Loss: 0.5513 | 0.81 steps/s | Step: 167k |  \n",
            "| Epoch: 13/27 (819/819) | Loss: 0.5512 | 0.82 steps/s | Step: 168k |  \n",
            "| Epoch: 14/27 (819/819) | Loss: 0.5510 | 0.82 steps/s | Step: 169k |  \n",
            "| Epoch: 15/27 (819/819) | Loss: 0.5505 | 0.83 steps/s | Step: 170k |  \n",
            "| Epoch: 16/27 (819/819) | Loss: 0.5502 | 0.83 steps/s | Step: 171k |  \n",
            "| Epoch: 17/27 (819/819) | Loss: 0.5499 | 0.84 steps/s | Step: 172k |  \n",
            "| Epoch: 18/27 (819/819) | Loss: 0.5497 | 0.83 steps/s | Step: 172k |  \n",
            "| Epoch: 19/27 (819/819) | Loss: 0.5496 | 0.83 steps/s | Step: 173k |  \n",
            "| Epoch: 20/27 (819/819) | Loss: 0.5493 | 0.83 steps/s | Step: 174k |  \n",
            "| Epoch: 21/27 (819/819) | Loss: 0.5492 | 0.82 steps/s | Step: 175k |  \n",
            "| Epoch: 22/27 (819/819) | Loss: 0.5489 | 0.82 steps/s | Step: 176k |  \n",
            "| Epoch: 23/27 (819/819) | Loss: 0.5488 | 0.82 steps/s | Step: 176k |  \n",
            "| Epoch: 24/27 (143/819) | Loss: 0.5478 | 0.84 steps/s | Step: 177k | Traceback (most recent call last):\n",
            "  File \"train_tacotron.py\", line 201, in <module>\n",
            "    main()\n",
            "  File \"train_tacotron.py\", line 98, in main\n",
            "    tts_train_loop(paths, model, optimizer, train_set, lr, training_steps, attn_example)\n",
            "  File \"train_tacotron.py\", line 142, in tts_train_loop\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 307, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 156, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvBIBj34J9O7"
      },
      "source": [
        "!rm -rf /content/WaveRNN/checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjsaYM625hFV",
        "outputId": "c73765f5-e2e8-4f56-953e-6226e630ccb4"
      },
      "source": [
        "!python gen_tacotron.py --input_text \"People dream to have a modern house located in the city, however, I love my house in my peaceful hometown\" wavernn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Initialising WaveRNN Model...\n",
            "\n",
            "Trainable Parameters: 4.234M\n",
            "\n",
            "Initialising Tacotron Model...\n",
            "\n",
            "Trainable Parameters: 11.088M\n",
            "+----------+---+--------------+---------+-----------------+----------------+-----------------+\n",
            "| Tacotron | r | Vocoder Type | WaveRNN | Generation Mode | Target Samples | Overlap Samples |\n",
            "+----------+---+--------------+---------+-----------------+----------------+-----------------+\n",
            "|   126k   | 2 |   WaveRNN    |  797k   |     Batched     |     11000      |       550       |\n",
            "+----------+---+--------------+---------+-----------------+----------------+-----------------+\n",
            " \n",
            "\n",
            "| Generating 1/1\n",
            "| ████████████████ 576000/580800 | Batch Size: 48 | Gen Rate: 70.4kHz | \n",
            "\n",
            "Done.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V-TTDJ9AiDe"
      },
      "source": [
        "!rm -rf /content/WaveRNN/checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXxocyWEBOy0"
      },
      "source": [
        "!cp /content/WaveRNN/checkpoints/ljspeech_lsa_smooth_attention.tacotron/taco_step154K_optim.pyt /content/drive/MyDrive/NLP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_kJ99MCMJbX"
      },
      "source": [
        "!cp /content/drive/MyDrive/NLP/checkpoints/ljspeech_lsa_smooth_attention.tacotron/latest_weights.pyt /content/WaveRNN/checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-qRgcOADBk5",
        "outputId": "3784cabd-05d7-42c3-df0b-1b230f97187e"
      },
      "source": [
        "!unzip '/content/WaveRNN/pretrained/ljspeech.wavernn.mol.800k.zip' -d '/content/WaveRNN/checkpoints/ljspeech_smothh.wavernn'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/WaveRNN/pretrained/ljspeech.wavernn.mol.800k.zip\n",
            "  inflating: /content/WaveRNN/checkpoints/ljspeech_smothh.wavernn/latest_weights.pyt  \n"
          ]
        }
      ]
    }
  ]
}